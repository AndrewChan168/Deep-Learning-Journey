
# 1. introduction
Transformers are deep learning models that excel in handling sequence-to-sequence prediction, outperforming older model like RNNs.

Transformer strength is effectively understanding the relationships between elements in input and output sequences over long distances. 

The central aspect of Transformer architecture is attention mechanism which assesses the relationship between words in a sequence by assigning weights, determining the degree of relatedness in meaning among owrds based on the training data. Transformers discern word meanings based on their surrounding context.

# 2. Architecture of Transformer
![Architecture](../graphs/transformer-architecture.jpg)

start from bottom to top

## 2.1 Encoder & Decoder

## 2.2 Embedding

## 2.3 Postion Embedding

## 2.4 Attention

