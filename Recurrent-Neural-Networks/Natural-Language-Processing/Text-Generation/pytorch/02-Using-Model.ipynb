{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d03aed-3cdc-4c97-a4de-12d81104f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b6bbb3-2396-4400-a755-11bd570331d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchModel import NextTokenLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8415574a-d2b4-4116-82b8-f4d45faf3b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NextTokenLSTM(\n",
      "  (embedding): Embedding(14094, 128)\n",
      "  (lstm): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2)\n",
      "  (full_connected): Linear(in_features=128, out_features=14094, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NextTokenLSTM(len(token_to_int)).to(device)\n",
    "model.load_state_dict(torch.load(\"models/wordLSTM.pth\", weights_only=True))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb8981f-3b0a-4492-b81c-6787f219c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae729484-a529-4e9c-abb4-4a9c78057465",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/token_to_int.pkl\", \"rb\") as file:\n",
    "    token_to_int=pickle.load(file)\n",
    "\n",
    "with open(\"data/int_to_token.pkl\", \"rb\") as file:\n",
    "    int_to_token=pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539fb95e-f640-4717-83d9-f39f59711d78",
   "metadata": {},
   "source": [
    "## Simple sampling by result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf9512b-8d34-4484-825f-819364885d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a2df32-0a4c-4dc8-8e49-ff1198d55847",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 100 # how many time step for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aab66f3-63c9-4956-95c3-87d3efec5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, sen_start, max_len=200):\n",
    "    model.eval()\n",
    "    text = sen_start.lower().split(' ')\n",
    "    hc = model.initalize_hidden(1)\n",
    "    length = max_len - len(text)\n",
    "    for _ in range(0, length):\n",
    "        if len(text) <= SEQ_LEN:\n",
    "            x = torch.tensor([[token_to_int[w] for w in text]])\n",
    "        else:\n",
    "            x = torch.tensor([[token_to_int[w] for w in text[-SEQ_LEN:]]])\n",
    "        inputs = x.to(device)\n",
    "        outputs, hc = model(inputs, hc)\n",
    "        logits = outputs[0][-1]\n",
    "        prob = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
    "        idx = np.random.choice(len(logits), p=prob)\n",
    "        text.append(int_to_token[idx])\n",
    "    sentence = \" \".join(text)\n",
    "    for m in \",.:;?!$()/_&%*@'`\":\n",
    "        sentence = sentence.replace(f\" {m}\", f\"{m} \")\n",
    "    sentence = sentence.replace('\"  ', '\"')\n",
    "    sentence = sentence.replace(\"'  \", \"'\")\n",
    "    sentence = sentence.replace('\" ', '\"')\n",
    "    sentence = sentence.replace(\"' \", \"'\")\n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766b4df5-e11c-4be3-b218-694202043aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince had explained to him,  in the habitual world with which they would not be made in what a new secretary,  and he began so much he saw the bell.  the culprit mihail clerk had been lighted and too half the acre,  he was interrupted.  \"sappho says seryozha too very exquisite,  \"he said to levin.  \"i don't know advice about you,  \"said sergey ivanovitch,  smiling.  he stood about,  and he walked away and saw.  one past later.  \"\"no,  that's it?  will you see or not for you?  why?  it's a foolish time my weakness but a man in nature very.  it's a wonder before me my heart,  completely exalted,  but not i'm taking a creature coming to more family yesterday.  i told you about his book,  you have refused to do,  and you're not pleased in being any points,  and i didn't want to take their conversation,  then we shall have a\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(sample(model, sen_start='Anna and the prince'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d56b5e0-c8c5-45e2-8886-cd6e62de8595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince should not be.  \"can the widow knife to bed again themselves.  those young landowners, 've been in a bad german,  \"vronsky seemed to take an acquaintance as he sat down in silence,  looking at his elbows with anna,  \"then there's no hurry for!  what is it?  \"(  anna expressed an answer that the children would have heard balls the day in a shadow.  that day a straw sight of it.  \"what is it?  thank starting?  \"\"that business didn't let or be so much.  \"vronsky was standing at getting up to stop and to stop herself.  \"how do you care?  \"she said,  \"i believe it's selfishness on her tiny little coat.  that's certain.  come,  please,  sir,  \"said alexey alexandrovitch,  getting up.  giving him several seconds he tried to find the pleasure when everything was allowed to sound,  the kitchen,  some one left up the staircase,  the feeling that had a\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(52)\n",
    "np.random.seed(52)\n",
    "print(sample(model, sen_start='Anna and the prince'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1350c-7120-4744-848c-bfa8a191314d",
   "metadata": {},
   "source": [
    "## Simple sampling by top-k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddc63683-53d7-419a-812b-8063e0e4e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_sample(model, sen_start, top_k, temperature=1, max_len=200):\n",
    "    model.eval()\n",
    "    text = sen_start.lower().split(' ')\n",
    "    hc = model.initalize_hidden(1)\n",
    "    length = max_len - len(text)\n",
    "    for _ in range(0, length):\n",
    "        if len(text) <= SEQ_LEN:\n",
    "            x = torch.tensor([[token_to_int[w] for w in text]])\n",
    "        else:\n",
    "            x = torch.tensor([[token_to_int[w] for w in text[-SEQ_LEN:]]])\n",
    "        inputs = x.to(device)\n",
    "        outputs, hc = model(inputs, hc)\n",
    "        logits = outputs[0][-1]\n",
    "        logits = logits/temperature\n",
    "        #prob = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n",
    "        prob = nn.functional.softmax(logits, dim=0).detach().cpu()\n",
    "        ps, tops = prob.topk(top_k)\n",
    "        ps = ps/ps.sum()\n",
    "        idx = np.random.choice(tops, p=ps.numpy())\n",
    "        text.append(int_to_token[idx])\n",
    "    sentence = \" \".join(text)\n",
    "    for m in \",.:;?!$()/_&%*@'`\":\n",
    "        sentence = sentence.replace(f\" {m}\", f\"{m} \")\n",
    "    sentence = sentence.replace('\"  ', '\"')\n",
    "    sentence = sentence.replace(\"'  \", \"'\")\n",
    "    sentence = sentence.replace('\" ', '\"')\n",
    "    sentence = sentence.replace(\"' \", \"'\")\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbe36da5-f7ce-4096-95be-4381dbfdb914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince had to leave her husband.  she knew that his mother was not married,  and would have liked to be married.  she was at ease with him.  she knew that her winter,  her father,  who had been going in the winter,  she was not in returning on his mind.  she was sitting in the middle of the room,  and the nurse came in her eyes,  and she saw that she was ashamed of.  she was glad of it,  and could not admit that in her soul she could not help following her.  the tears and happiness in her eyes,  revived it unseen expression of brilliance,  cruel aversion,  prevented him,  and squeezing his hand,  and with the same despondent face gazed at her.  vronsky,  with a long overcoat,  gazed at the looking-glass,  and then with a smile of her eyes,  made something thin,  and was appalled at her movements.  she was not calm,  but was embarrassed with her.  the smile was drawn by her childish eyes.  she saw the sound\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(topk_sample(model, sen_start='Anna and the prince', top_k=3,temperature=0.5 )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cee691e8-9ded-4f19-8356-ef9b63480d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anna and the prince had explained to her.  but he was a feeling that he was not to blame.  he was not to blame for this instant,  and he had been a long while before,  though he was unmanly and effeminate,  effeminate,  and lack of wealthy significance.  \"you've been so fond of your sister,  \"said stepan arkadyevitch,  smiling.  \"i'm going to the bee house,  i'll send you a shirt.  \"\"i'm not afraid you've been here for me.  \"\"well,  then,  i'm not a negro. . .  \"\"oh,  well,  then,  \"he said to himself,  and went into the room.  \"i don't want to go to bryansky,  and i'll go and see him.  \"\"oh,  i'm going on,  \"said the englishman.  \"i've come to fetch you.  \"\"well,  what do you say to pyramids?  \"\"oh,  i\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(52)\n",
    "np.random.seed(52)\n",
    "print(topk_sample(model, sen_start='Anna and the prince', top_k=3,temperature=0.5 )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a4878-5964-4c0a-aa8d-c3a79a674ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
