{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffde5da8-7ebd-4f99-8121-b5c171fa47f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb0a9ff-6292-4479-a019-5afbd56e99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde82c0b-e18d-4070-a513-413f86831e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter', '1\\n\\n\\nHappy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own\\nway.\\n\\nEverything', 'was', 'in', 'confusion', 'in', 'the', \"Oblonskys'\"]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/anna.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "words = text.split(\" \")\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf6fccb-aa11-45fc-8bae-00aa37370150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1', '', '', 'happy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way.', '', 'everything']\n"
     ]
    }
   ],
   "source": [
    "clean_text = text.lower().replace(\"\\n\", \" \")\n",
    "words = clean_text.split(\" \")\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09498366-0a78-4a3a-9bd0-b7476ee5e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1', '', '', 'happy', 'families', 'are', 'all', 'alike', ';', '', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way']\n"
     ]
    }
   ],
   "source": [
    "for x in \",.:;?!$()/_&%*@'`\":\n",
    "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
    "clean_text = clean_text.replace('\"', ' \" ')\n",
    "words = clean_text.split(\" \")\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc2a3af-56d7-4507-9009-f4f380b245ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = clean_text.split()\n",
    "print(clean_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519b362d-eeb3-41a0-9947-cc6783b1e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555f26da-344f-4689-a504-2c9a76b7ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/clean_tokens.pkl\", \"wb\") as file:\n",
    "    pickle.dump(clean_tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7956f508-5fd4-44b1-9585-86708ac1250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text containers 434880 tokens\n"
     ]
    }
   ],
   "source": [
    "tokens_length = len(clean_tokens)\n",
    "print(f\"The text containers {text_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c8ef638-768c-4df3-bf45-d6d8348fdfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens_counter = Counter(clean_tokens)\n",
    "sorted_tokens_counter = sorted(tokens_counter, key=tokens_counter.get, reverse=True)\n",
    "print(sorted_tokens_counter[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c80f7431-ff90-4ab5-b733-eb99b5097f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14094 unique tokens\n"
     ]
    }
   ],
   "source": [
    "unique_tokens_length = len(sorted_tokens_counter)\n",
    "print(f\"There are {unique_tokens_length} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffda866a-284b-4728-8c4f-81cbacf16c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_int = {v:k for k,v in enumerate(sorted_tokens_counter)}\n",
    "int_to_token = {k:v for k,v in enumerate(sorted_tokens_counter)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f50dbcc4-12a1-4e14-aa0a-c71f937e96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/token_to_int.pkl\", \"wb\") as file:\n",
    "    pickle.dump(token_to_int, file)\n",
    "\n",
    "with open(\"data/int_to_token.pkl\", \"wb\") as file:\n",
    "    pickle.dump(int_to_token, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9547d2-bab9-474c-9bbd-a24cc24a6cc9",
   "metadata": {},
   "source": [
    "## Create batches of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72db5bf7-1b35-410e-b074-61dbbe9ef64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [token_to_int[t] for t in clean_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f947562a-8bfb-4f89-8ccf-c73f995780c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 100\n",
    "xy_pairs = []\n",
    "for idx in range(0, len(token_ids)-SEQ_LEN-1):\n",
    "    x = token_ids[idx:idx+SEQ_LEN]\n",
    "    y = token_ids[idx+1:idx+SEQ_LEN+1]\n",
    "    xy_pairs.append((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75cfc658-2970-484b-8e73-6da4a58d3405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[206, 2729, 279, 2945, 83, 31, 2533, 35, 201, 675]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_pairs[0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fa079cd-e847-4867-8c7e-69a1bc1081b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2729, 279, 2945, 83, 31, 2533, 35, 201, 675, 364]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_pairs[0][1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "583f68ef-439f-4499-80b9-17a7b4ccabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/XY_pairs.pkl\", \"wb\") as file:\n",
    "    pickle.dump(xy_pairs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7332adde-1d6a-48a8-b13b-1f9f5cbbcbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
