{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5521bb85-2693-4072-97fe-1220abdde365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d9d367-2389-4694-ad27-e9370a88fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12e239f-ff10-48fc-9ff1-007ec72378a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocesses import CustomCocoDataset, TokenIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c356272-12c9-4cd0-9935-9cfce4140ca7",
   "metadata": {},
   "source": [
    "# Define Encoder & Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88382510-c8cc-4506-9db2-8b225cb81dcf",
   "metadata": {},
   "source": [
    "## CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89bf8bb-dd9e-4629-98ca-c5d989b86a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        resnet = models.resnet50(weights=True)\n",
    "        layers = list(resnet.children())[:-1] # remove last full connected layer\n",
    "        self.resnet_base = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet_base(img)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        final_features = self.batch_norm(self.fc(features))\n",
    "        return final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2d2dc-a884-44bf-84d0-b6f2064d76fa",
   "metadata": {},
   "source": [
    "## LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69e64ed-7b43-4e96-aa2b-a05a049176bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, input_embeds, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeds = self.embedding_layer(captions)\n",
    "        embeds = torch.cat((input_embeds.unsqueeze(1), embeds), 1)\n",
    "        rnn_inputs = pack_padded_sequence(embeds, lengths, batch_first=True) \n",
    "        hidden_outputs, _ = self.lstm_layer(rnn_inputs)\n",
    "        outputs = self.linear_layer(hidden_outputs)\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, input_embeds, rnn_states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        token_idxs = []\n",
    "        rnn_inputs = input_embeds.unsqueeze(1)\n",
    "        for idx in range(self.max_seq_len):\n",
    "            hidden_outputs, rnn_states = self.lstm_layer(rnn_inputs, rnn_states) # hidden: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear_layer(hidden_outputs.squeeze(1))\n",
    "            _, predictions = outputs.max(1)\n",
    "            token_idxs.append(predictions)\n",
    "            rnn_inputs = (self.embedding_layer(predictions)).unsqueeze(1)\n",
    "        return torch.stack(token_idxs, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e0cc1-0fc5-4f7f-b88d-b78c28ab6f1b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b260ca-14e1-4fa6-a12b-c666ecc72e0b",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4d28f-41fb-4a03-8ef9-1469f7062015",
   "metadata": {},
   "source": [
    "create the model directory as check point for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c32d67-2ffa-481b-a33b-98f91d121290",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ba5cd-09cc-4166-a586-0d9120e6e88c",
   "metadata": {},
   "source": [
    "load pre-defined Token2Index instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874db4f2-a4e2-46b5-83cf-b2ace71ff0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<preprocesses.TokenIndex at 0x728ba7ddbd90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('token_index.pkl', 'rb') as file:\n",
    "    token2index = pickle.load(file)\n",
    "\n",
    "token2index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774536ab-f579-4042-97ab-9d6c9bf83b88",
   "metadata": {},
   "source": [
    "define transformation pipeline for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "392da338-d4d8-4c87-87c4-63a09764a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb433479-8faf-4b4d-88a9-a2625c78477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.86s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<preprocesses.CustomCocoDataset at 0x728ba7307be0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_dataset = CustomCocoDataset(\"data/reshaped_train2014\", \"data/annotations/captions_train2014.json\", token2index, transform_pipeline)\n",
    "coco_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0435d26-3db4-4dc0-bca9-27796adbd345",
   "metadata": {},
   "source": [
    "define `collate_fn` for Pytorch **DataLoader**. In PyTorch, the collate_fn is a function used by the DataLoader to specify how a list of data samples (from the dataset) should be merged into a single batch. It is particularly useful when working with datasets that have variable-length inputs or require custom preprocessing during batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c1bf2e-8bdc-47a7-958d-d3b9a07ccb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "\n",
    "    we build our own custom collate_fn because  merging caption (including padding) \n",
    "    is not supported in default collate_fn\n",
    "    Args:\n",
    "        batch: list of tuple (image, caption).\n",
    "                - image: torch tensor of shape (3, 256, 256).\n",
    "                - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    batch.sort(key=lambda d: len(d[1]), reverse=True)\n",
    "    imgs, caps = zip(*batch)\n",
    "\n",
    "    # Merge images (from list of 3D tensors to 4D tensor).\n",
    "    # Originally, imgs is a list of <batch_size> number of RGB images with dimensions (3, 256, 256)\n",
    "    #Â This line of code turns it into a single tensor of dimensions (<batch_size>, 3, 256, 256)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "\n",
    "    # Merge captions (from list of 1D tensors to 2D tensor), similar to merging of images donw above.\n",
    "    cap_lens = [len(cap) for cap in caps]\n",
    "    targets = torch.zeros(len(caps), max(cap_lens)).long()\n",
    "    for i, cap in enumerate(caps):\n",
    "        end = cap_lens[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return imgs, targets, cap_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3193211c-c150-4626-903b-790559442668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1618 batches in each epochs\n"
     ]
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=coco_dataset,batch_size=256,shuffle=True,collate_fn=collate_function)\n",
    "print(f\"There are {len(data_loader)} batches in each epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba4d23-d96d-465b-9400-d8a4b17a7405",
   "metadata": {},
   "source": [
    "## Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1f5196d-2474-41de-9c7f-447eb2039db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code type using for Pytorch: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Code type using for Pytorch: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95719b06-6d83-4fdd-8eae-191ee3b2134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    }
   ],
   "source": [
    "encoder = CNNEncoder(256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78134785-db43-430d-9cbd-f895ed4ee302",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RNNModel(256, 512, len(token2index), 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d875a1-fe4d-47a4-8c47-993cb5f1f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "parameters = list(decoder.parameters()) + list(encoder.linear_layer.parameters()) + list(encoder.batch_norm.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68874fa-41fc-4c2e-8ee5-80e6902f0261",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8916b8-b35d-4288-9e8d-ec9d578cbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = len(data_loader)\n",
    "epoch_num = 10\n",
    "model_path = os.path.join('models',)\n",
    "for epoch in range(epoch_num):\n",
    "    for id_, (imgs, caps, lens) in enumerate(data_loader):\n",
    "        # move batch to device\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(imgs)\n",
    "        outputs = decoder(features, caps, lens)\n",
    "        losses = loss_criterion(outputs, tgts)\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print log\n",
    "        if id_%100 == 0: \n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'.format(epoch, 5, i, batch_num, losses.item(), np.exp(losses.item())))\n",
    "            \n",
    "    # Save the model checkpoints\n",
    "    torch.save(decoder.state_dict(), os.path.join('models', 'decoder-{}.ckpt'.format(epoch)))\n",
    "    torch.save(encoder.state_dict(), os.path.join('models', 'encoder-{}.ckpt'.format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccbe51-2a61-49b6-a8ce-92ce41d5d32b",
   "metadata": {},
   "source": [
    "save the final decode and encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad79e5-08b4-4c33-8259-14ff8e7fcdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), os.path.join('models', 'decoder.ckpt'))\n",
    "torch.save(encoder.state_dict(), os.path.join('models', 'encoder.ckpt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
