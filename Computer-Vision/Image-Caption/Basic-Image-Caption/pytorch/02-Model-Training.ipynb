{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5521bb85-2693-4072-97fe-1220abdde365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d9d367-2389-4694-ad27-e9370a88fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12e239f-ff10-48fc-9ff1-007ec72378a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocesses import CustomCocoDataset, TokenIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c356272-12c9-4cd0-9935-9cfce4140ca7",
   "metadata": {},
   "source": [
    "# Define Encoder & Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88382510-c8cc-4506-9db2-8b225cb81dcf",
   "metadata": {},
   "source": [
    "## CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f89bf8bb-dd9e-4629-98ca-c5d989b86a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        resnet = models.resnet50(weights=True)\n",
    "        layers = list(resnet.children())[:-1] # remove last full connected layer\n",
    "        self.resnet_base = nn.Sequential(*layers)\n",
    "        self.linear_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet_base(img)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        final_features = self.batch_norm(self.linear_layer(features))\n",
    "        return final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2d2dc-a884-44bf-84d0-b6f2064d76fa",
   "metadata": {},
   "source": [
    "## LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a69e64ed-7b43-4e96-aa2b-a05a049176bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, input_embeds, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeds = self.embedding_layer(captions)\n",
    "        embeds = torch.cat((input_embeds.unsqueeze(1), embeds), 1)\n",
    "        rnn_inputs = pack_padded_sequence(embeds, lengths, batch_first=True) \n",
    "        hidden_outputs, _ = self.lstm_layer(rnn_inputs)\n",
    "        outputs = self.linear_layer(hidden_outputs[0])\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, input_embeds, rnn_states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        token_idxs = []\n",
    "        rnn_inputs = input_embeds.unsqueeze(1)\n",
    "        for idx in range(self.max_seq_len):\n",
    "            hidden_outputs, rnn_states = self.lstm_layer(rnn_inputs, rnn_states) # hidden: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear_layer(hidden_outputs.squeeze(1))\n",
    "            _, predictions = outputs.max(1)\n",
    "            token_idxs.append(predictions)\n",
    "            rnn_inputs = (self.embedding_layer(predictions)).unsqueeze(1)\n",
    "        return torch.stack(token_idxs, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e0cc1-0fc5-4f7f-b88d-b78c28ab6f1b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b260ca-14e1-4fa6-a12b-c666ecc72e0b",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4d28f-41fb-4a03-8ef9-1469f7062015",
   "metadata": {},
   "source": [
    "create the model directory as check point for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7c32d67-2ffa-481b-a33b-98f91d121290",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ba5cd-09cc-4166-a586-0d9120e6e88c",
   "metadata": {},
   "source": [
    "load pre-defined Token2Index instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "874db4f2-a4e2-46b5-83cf-b2ace71ff0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<preprocesses.TokenIndex at 0x7e36c7d48a90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('token_index.pkl', 'rb') as file:\n",
    "    token2index = pickle.load(file)\n",
    "\n",
    "token2index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774536ab-f579-4042-97ab-9d6c9bf83b88",
   "metadata": {},
   "source": [
    "define transformation pipeline for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "392da338-d4d8-4c87-87c4-63a09764a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb433479-8faf-4b4d-88a9-a2625c78477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.83s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<preprocesses.CustomCocoDataset at 0x7e361f873f10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_dataset = CustomCocoDataset(\"data/reshaped_train2014\", \"data/annotations/captions_train2014.json\", token2index, transform_pipeline)\n",
    "coco_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0435d26-3db4-4dc0-bca9-27796adbd345",
   "metadata": {},
   "source": [
    "define `collate_fn` for Pytorch **DataLoader**. In PyTorch, the collate_fn is a function used by the DataLoader to specify how a list of data samples (from the dataset) should be merged into a single batch. It is particularly useful when working with datasets that have variable-length inputs or require custom preprocessing during batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2c1bf2e-8bdc-47a7-958d-d3b9a07ccb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "\n",
    "    we build our own custom collate_fn because  merging caption (including padding) \n",
    "    is not supported in default collate_fn\n",
    "    Args:\n",
    "        batch: list of tuple (image, caption).\n",
    "                - image: torch tensor of shape (3, 256, 256).\n",
    "                - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    batch.sort(key=lambda d: len(d[1]), reverse=True)\n",
    "    imgs, caps = zip(*batch)\n",
    "\n",
    "    # Merge images (from list of 3D tensors to 4D tensor).\n",
    "    # Originally, imgs is a list of <batch_size> number of RGB images with dimensions (3, 256, 256)\n",
    "    #Â This line of code turns it into a single tensor of dimensions (<batch_size>, 3, 256, 256)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "\n",
    "    # Merge captions (from list of 1D tensors to 2D tensor), similar to merging of images donw above.\n",
    "    cap_lens = [len(cap) for cap in caps]\n",
    "    targets = torch.zeros(len(caps), max(cap_lens)).long()\n",
    "    for i, cap in enumerate(caps):\n",
    "        end = cap_lens[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return imgs, targets, cap_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3193211c-c150-4626-903b-790559442668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1618 batches in each epochs\n"
     ]
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=coco_dataset,batch_size=256,shuffle=True,collate_fn=collate_function)\n",
    "print(f\"There are {len(data_loader)} batches in each epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba4d23-d96d-465b-9400-d8a4b17a7405",
   "metadata": {},
   "source": [
    "## Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1f5196d-2474-41de-9c7f-447eb2039db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code type using for Pytorch: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Code type using for Pytorch: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95719b06-6d83-4fdd-8eae-191ee3b2134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNNEncoder(256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78134785-db43-430d-9cbd-f895ed4ee302",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RNNModel(256, 512, len(token2index), 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79d875a1-fe4d-47a4-8c47-993cb5f1f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "parameters = list(decoder.parameters()) + list(encoder.linear_layer.parameters()) + list(encoder.batch_norm.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68874fa-41fc-4c2e-8ee5-80e6902f0261",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a8916b8-b35d-4288-9e8d-ec9d578cbb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5], Step [0/1618], Loss: 8.9880, Perplexity: 8006.6793\n",
      "Epoch [0/5], Step [100/1618], Loss: 3.7494, Perplexity: 42.4946\n",
      "Epoch [0/5], Step [200/1618], Loss: 3.3279, Perplexity: 27.8801\n",
      "Epoch [0/5], Step [300/1618], Loss: 3.0066, Perplexity: 20.2182\n",
      "Epoch [0/5], Step [400/1618], Loss: 2.9582, Perplexity: 19.2630\n",
      "Epoch [0/5], Step [500/1618], Loss: 2.6639, Perplexity: 14.3517\n",
      "Epoch [0/5], Step [600/1618], Loss: 2.6400, Perplexity: 14.0135\n",
      "Epoch [0/5], Step [700/1618], Loss: 2.6156, Perplexity: 13.6749\n",
      "Epoch [0/5], Step [800/1618], Loss: 2.6118, Perplexity: 13.6235\n",
      "Epoch [0/5], Step [900/1618], Loss: 2.6401, Perplexity: 14.0147\n",
      "Epoch [0/5], Step [1000/1618], Loss: 2.3962, Perplexity: 10.9809\n",
      "Epoch [0/5], Step [1100/1618], Loss: 2.4257, Perplexity: 11.3097\n",
      "Epoch [0/5], Step [1200/1618], Loss: 2.4685, Perplexity: 11.8052\n",
      "Epoch [0/5], Step [1300/1618], Loss: 2.4187, Perplexity: 11.2314\n",
      "Epoch [0/5], Step [1400/1618], Loss: 2.2768, Perplexity: 9.7454\n",
      "Epoch [0/5], Step [1500/1618], Loss: 2.3733, Perplexity: 10.7327\n",
      "Epoch [0/5], Step [1600/1618], Loss: 2.2771, Perplexity: 9.7480\n",
      "Epoch [1/5], Step [0/1618], Loss: 2.2956, Perplexity: 9.9308\n",
      "Epoch [1/5], Step [100/1618], Loss: 2.3444, Perplexity: 10.4269\n",
      "Epoch [1/5], Step [200/1618], Loss: 2.1949, Perplexity: 8.9795\n",
      "Epoch [1/5], Step [300/1618], Loss: 2.2263, Perplexity: 9.2655\n",
      "Epoch [1/5], Step [400/1618], Loss: 2.3018, Perplexity: 9.9917\n",
      "Epoch [1/5], Step [500/1618], Loss: 2.2503, Perplexity: 9.4902\n",
      "Epoch [1/5], Step [600/1618], Loss: 2.2675, Perplexity: 9.6548\n",
      "Epoch [1/5], Step [700/1618], Loss: 2.1180, Perplexity: 8.3147\n",
      "Epoch [1/5], Step [800/1618], Loss: 2.1699, Perplexity: 8.7578\n",
      "Epoch [1/5], Step [900/1618], Loss: 2.2106, Perplexity: 9.1214\n",
      "Epoch [1/5], Step [1000/1618], Loss: 2.1349, Perplexity: 8.4565\n",
      "Epoch [1/5], Step [1100/1618], Loss: 2.1719, Perplexity: 8.7749\n",
      "Epoch [1/5], Step [1200/1618], Loss: 2.2370, Perplexity: 9.3656\n",
      "Epoch [1/5], Step [1300/1618], Loss: 2.1869, Perplexity: 8.9076\n",
      "Epoch [1/5], Step [1400/1618], Loss: 2.0907, Perplexity: 8.0905\n",
      "Epoch [1/5], Step [1500/1618], Loss: 2.1825, Perplexity: 8.8684\n",
      "Epoch [1/5], Step [1600/1618], Loss: 2.2013, Perplexity: 9.0369\n",
      "Epoch [2/5], Step [0/1618], Loss: 2.0575, Perplexity: 7.8266\n",
      "Epoch [2/5], Step [100/1618], Loss: 2.1571, Perplexity: 8.6460\n",
      "Epoch [2/5], Step [200/1618], Loss: 2.0480, Perplexity: 7.7527\n",
      "Epoch [2/5], Step [300/1618], Loss: 1.9900, Perplexity: 7.3153\n",
      "Epoch [2/5], Step [400/1618], Loss: 2.1115, Perplexity: 8.2607\n",
      "Epoch [2/5], Step [500/1618], Loss: 2.0978, Perplexity: 8.1481\n",
      "Epoch [2/5], Step [600/1618], Loss: 2.0971, Perplexity: 8.1425\n",
      "Epoch [2/5], Step [700/1618], Loss: 2.0964, Perplexity: 8.1367\n",
      "Epoch [2/5], Step [800/1618], Loss: 2.1109, Perplexity: 8.2556\n",
      "Epoch [2/5], Step [900/1618], Loss: 2.0976, Perplexity: 8.1464\n",
      "Epoch [2/5], Step [1000/1618], Loss: 2.0181, Perplexity: 7.5240\n",
      "Epoch [2/5], Step [1100/1618], Loss: 2.0094, Perplexity: 7.4589\n",
      "Epoch [2/5], Step [1200/1618], Loss: 2.0367, Perplexity: 7.6654\n",
      "Epoch [2/5], Step [1300/1618], Loss: 2.1099, Perplexity: 8.2473\n",
      "Epoch [2/5], Step [1400/1618], Loss: 2.1045, Perplexity: 8.2032\n",
      "Epoch [2/5], Step [1500/1618], Loss: 2.0638, Perplexity: 7.8762\n",
      "Epoch [2/5], Step [1600/1618], Loss: 2.1240, Perplexity: 8.3647\n",
      "Epoch [3/5], Step [0/1618], Loss: 1.9507, Perplexity: 7.0333\n",
      "Epoch [3/5], Step [100/1618], Loss: 1.9799, Perplexity: 7.2417\n",
      "Epoch [3/5], Step [200/1618], Loss: 2.0425, Perplexity: 7.7100\n",
      "Epoch [3/5], Step [300/1618], Loss: 1.9695, Perplexity: 7.1673\n",
      "Epoch [3/5], Step [400/1618], Loss: 2.0328, Perplexity: 7.6357\n",
      "Epoch [3/5], Step [500/1618], Loss: 1.9596, Perplexity: 7.0964\n",
      "Epoch [3/5], Step [600/1618], Loss: 1.9309, Perplexity: 6.8956\n",
      "Epoch [3/5], Step [700/1618], Loss: 1.9774, Perplexity: 7.2242\n",
      "Epoch [3/5], Step [800/1618], Loss: 1.9941, Perplexity: 7.3455\n",
      "Epoch [3/5], Step [900/1618], Loss: 2.0014, Perplexity: 7.3993\n",
      "Epoch [3/5], Step [1000/1618], Loss: 1.9316, Perplexity: 6.9002\n",
      "Epoch [3/5], Step [1100/1618], Loss: 1.9603, Perplexity: 7.1013\n",
      "Epoch [3/5], Step [1200/1618], Loss: 1.9724, Perplexity: 7.1880\n",
      "Epoch [3/5], Step [1300/1618], Loss: 2.0205, Perplexity: 7.5418\n",
      "Epoch [3/5], Step [1400/1618], Loss: 2.0342, Perplexity: 7.6464\n",
      "Epoch [3/5], Step [1500/1618], Loss: 1.8949, Perplexity: 6.6521\n",
      "Epoch [3/5], Step [1600/1618], Loss: 1.9162, Perplexity: 6.7951\n",
      "Epoch [4/5], Step [0/1618], Loss: 1.9410, Perplexity: 6.9659\n",
      "Epoch [4/5], Step [100/1618], Loss: 1.9092, Perplexity: 6.7475\n",
      "Epoch [4/5], Step [200/1618], Loss: 1.8814, Perplexity: 6.5628\n",
      "Epoch [4/5], Step [300/1618], Loss: 1.9108, Perplexity: 6.7583\n",
      "Epoch [4/5], Step [400/1618], Loss: 1.8415, Perplexity: 6.3061\n",
      "Epoch [4/5], Step [500/1618], Loss: 1.9295, Perplexity: 6.8861\n",
      "Epoch [4/5], Step [600/1618], Loss: 1.8856, Perplexity: 6.5902\n",
      "Epoch [4/5], Step [700/1618], Loss: 1.8377, Perplexity: 6.2822\n",
      "Epoch [4/5], Step [800/1618], Loss: 1.9543, Perplexity: 7.0586\n",
      "Epoch [4/5], Step [900/1618], Loss: 1.9730, Perplexity: 7.1921\n",
      "Epoch [4/5], Step [1000/1618], Loss: 1.9004, Perplexity: 6.6883\n",
      "Epoch [4/5], Step [1100/1618], Loss: 1.9938, Perplexity: 7.3430\n",
      "Epoch [4/5], Step [1200/1618], Loss: 1.9470, Perplexity: 7.0079\n",
      "Epoch [4/5], Step [1300/1618], Loss: 1.8906, Perplexity: 6.6236\n",
      "Epoch [4/5], Step [1400/1618], Loss: 1.8977, Perplexity: 6.6705\n",
      "Epoch [4/5], Step [1500/1618], Loss: 1.9997, Perplexity: 7.3866\n",
      "Epoch [4/5], Step [1600/1618], Loss: 1.9311, Perplexity: 6.8970\n",
      "Epoch [5/5], Step [0/1618], Loss: 1.9010, Perplexity: 6.6928\n",
      "Epoch [5/5], Step [100/1618], Loss: 1.7468, Perplexity: 5.7360\n",
      "Epoch [5/5], Step [200/1618], Loss: 1.8308, Perplexity: 6.2390\n",
      "Epoch [5/5], Step [300/1618], Loss: 1.8234, Perplexity: 6.1929\n",
      "Epoch [5/5], Step [400/1618], Loss: 1.8733, Perplexity: 6.5096\n",
      "Epoch [5/5], Step [500/1618], Loss: 1.8793, Perplexity: 6.5488\n",
      "Epoch [5/5], Step [600/1618], Loss: 1.8565, Perplexity: 6.4014\n",
      "Epoch [5/5], Step [700/1618], Loss: 1.9436, Perplexity: 6.9836\n",
      "Epoch [5/5], Step [800/1618], Loss: 1.8730, Perplexity: 6.5080\n",
      "Epoch [5/5], Step [900/1618], Loss: 1.8811, Perplexity: 6.5606\n",
      "Epoch [5/5], Step [1000/1618], Loss: 1.8653, Perplexity: 6.4576\n",
      "Epoch [5/5], Step [1100/1618], Loss: 1.9098, Perplexity: 6.7520\n",
      "Epoch [5/5], Step [1200/1618], Loss: 1.8374, Perplexity: 6.2802\n",
      "Epoch [5/5], Step [1300/1618], Loss: 1.8510, Perplexity: 6.3659\n",
      "Epoch [5/5], Step [1400/1618], Loss: 1.8516, Perplexity: 6.3700\n",
      "Epoch [5/5], Step [1500/1618], Loss: 1.8803, Perplexity: 6.5557\n",
      "Epoch [5/5], Step [1600/1618], Loss: 1.9276, Perplexity: 6.8729\n",
      "Epoch [6/5], Step [0/1618], Loss: 1.7250, Perplexity: 5.6126\n",
      "Epoch [6/5], Step [100/1618], Loss: 1.8235, Perplexity: 6.1932\n",
      "Epoch [6/5], Step [200/1618], Loss: 1.7473, Perplexity: 5.7389\n",
      "Epoch [6/5], Step [300/1618], Loss: 1.7529, Perplexity: 5.7714\n",
      "Epoch [6/5], Step [400/1618], Loss: 1.7652, Perplexity: 5.8428\n",
      "Epoch [6/5], Step [500/1618], Loss: 1.8540, Perplexity: 6.3851\n",
      "Epoch [6/5], Step [600/1618], Loss: 1.7410, Perplexity: 5.7030\n",
      "Epoch [6/5], Step [700/1618], Loss: 1.8304, Perplexity: 6.2366\n",
      "Epoch [6/5], Step [800/1618], Loss: 1.7793, Perplexity: 5.9256\n",
      "Epoch [6/5], Step [900/1618], Loss: 1.8373, Perplexity: 6.2794\n",
      "Epoch [6/5], Step [1000/1618], Loss: 1.7954, Perplexity: 6.0216\n",
      "Epoch [6/5], Step [1100/1618], Loss: 1.8422, Perplexity: 6.3103\n",
      "Epoch [6/5], Step [1200/1618], Loss: 1.8464, Perplexity: 6.3367\n",
      "Epoch [6/5], Step [1300/1618], Loss: 1.8492, Perplexity: 6.3547\n",
      "Epoch [6/5], Step [1400/1618], Loss: 1.7873, Perplexity: 5.9734\n",
      "Epoch [6/5], Step [1500/1618], Loss: 1.9116, Perplexity: 6.7642\n",
      "Epoch [6/5], Step [1600/1618], Loss: 1.8342, Perplexity: 6.2602\n",
      "Epoch [7/5], Step [0/1618], Loss: 1.6904, Perplexity: 5.4218\n",
      "Epoch [7/5], Step [100/1618], Loss: 1.7998, Perplexity: 6.0485\n",
      "Epoch [7/5], Step [200/1618], Loss: 1.7095, Perplexity: 5.5262\n",
      "Epoch [7/5], Step [300/1618], Loss: 1.7275, Perplexity: 5.6265\n",
      "Epoch [7/5], Step [400/1618], Loss: 1.8042, Perplexity: 6.0753\n",
      "Epoch [7/5], Step [500/1618], Loss: 1.7433, Perplexity: 5.7161\n",
      "Epoch [7/5], Step [600/1618], Loss: 1.7857, Perplexity: 5.9639\n",
      "Epoch [7/5], Step [700/1618], Loss: 1.7864, Perplexity: 5.9676\n",
      "Epoch [7/5], Step [800/1618], Loss: 1.8348, Perplexity: 6.2640\n",
      "Epoch [7/5], Step [900/1618], Loss: 1.7224, Perplexity: 5.5977\n",
      "Epoch [7/5], Step [1000/1618], Loss: 1.7736, Perplexity: 5.8918\n",
      "Epoch [7/5], Step [1100/1618], Loss: 1.8490, Perplexity: 6.3534\n",
      "Epoch [7/5], Step [1200/1618], Loss: 1.8429, Perplexity: 6.3147\n",
      "Epoch [7/5], Step [1300/1618], Loss: 1.8262, Perplexity: 6.2100\n",
      "Epoch [7/5], Step [1400/1618], Loss: 1.7881, Perplexity: 5.9782\n",
      "Epoch [7/5], Step [1500/1618], Loss: 1.7850, Perplexity: 5.9594\n",
      "Epoch [7/5], Step [1600/1618], Loss: 1.8512, Perplexity: 6.3676\n",
      "Epoch [8/5], Step [0/1618], Loss: 1.6922, Perplexity: 5.4316\n",
      "Epoch [8/5], Step [100/1618], Loss: 1.7187, Perplexity: 5.5772\n",
      "Epoch [8/5], Step [200/1618], Loss: 1.7391, Perplexity: 5.6923\n",
      "Epoch [8/5], Step [300/1618], Loss: 1.7167, Perplexity: 5.5662\n",
      "Epoch [8/5], Step [400/1618], Loss: 1.7688, Perplexity: 5.8638\n",
      "Epoch [8/5], Step [500/1618], Loss: 1.7344, Perplexity: 5.6656\n",
      "Epoch [8/5], Step [600/1618], Loss: 1.7265, Perplexity: 5.6210\n",
      "Epoch [8/5], Step [700/1618], Loss: 1.7769, Perplexity: 5.9113\n",
      "Epoch [8/5], Step [800/1618], Loss: 1.6937, Perplexity: 5.4394\n",
      "Epoch [8/5], Step [900/1618], Loss: 1.8053, Perplexity: 6.0816\n",
      "Epoch [8/5], Step [1000/1618], Loss: 1.7932, Perplexity: 6.0084\n",
      "Epoch [8/5], Step [1100/1618], Loss: 1.7166, Perplexity: 5.5656\n",
      "Epoch [8/5], Step [1200/1618], Loss: 1.7692, Perplexity: 5.8664\n",
      "Epoch [8/5], Step [1300/1618], Loss: 1.7250, Perplexity: 5.6127\n",
      "Epoch [8/5], Step [1400/1618], Loss: 1.7677, Perplexity: 5.8576\n",
      "Epoch [8/5], Step [1500/1618], Loss: 1.8350, Perplexity: 6.2652\n",
      "Epoch [8/5], Step [1600/1618], Loss: 1.8358, Perplexity: 6.2702\n",
      "Epoch [9/5], Step [0/1618], Loss: 1.6545, Perplexity: 5.2305\n",
      "Epoch [9/5], Step [100/1618], Loss: 1.7218, Perplexity: 5.5948\n",
      "Epoch [9/5], Step [200/1618], Loss: 1.7040, Perplexity: 5.4957\n",
      "Epoch [9/5], Step [300/1618], Loss: 1.6660, Perplexity: 5.2911\n",
      "Epoch [9/5], Step [400/1618], Loss: 1.6999, Perplexity: 5.4735\n",
      "Epoch [9/5], Step [500/1618], Loss: 1.7292, Perplexity: 5.6363\n",
      "Epoch [9/5], Step [600/1618], Loss: 1.6990, Perplexity: 5.4686\n",
      "Epoch [9/5], Step [700/1618], Loss: 1.7143, Perplexity: 5.5529\n",
      "Epoch [9/5], Step [800/1618], Loss: 1.7260, Perplexity: 5.6182\n",
      "Epoch [9/5], Step [900/1618], Loss: 1.6734, Perplexity: 5.3301\n",
      "Epoch [9/5], Step [1000/1618], Loss: 1.7834, Perplexity: 5.9500\n",
      "Epoch [9/5], Step [1100/1618], Loss: 1.7452, Perplexity: 5.7270\n",
      "Epoch [9/5], Step [1200/1618], Loss: 1.8205, Perplexity: 6.1751\n",
      "Epoch [9/5], Step [1300/1618], Loss: 1.7575, Perplexity: 5.7978\n",
      "Epoch [9/5], Step [1400/1618], Loss: 1.7291, Perplexity: 5.6354\n",
      "Epoch [9/5], Step [1500/1618], Loss: 1.7097, Perplexity: 5.5271\n",
      "Epoch [9/5], Step [1600/1618], Loss: 1.7017, Perplexity: 5.4830\n"
     ]
    }
   ],
   "source": [
    "batch_num = len(data_loader)\n",
    "epoch_num = 10\n",
    "model_path = os.path.join('models',)\n",
    "for epoch in range(epoch_num):\n",
    "    for id_, (imgs, caps, lens) in enumerate(data_loader):\n",
    "        # move batch to device\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(imgs)\n",
    "        outputs = decoder(features, caps, lens)\n",
    "        losses = loss_criterion(outputs, tgts)\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print log\n",
    "        if id_%100 == 0: \n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'.format(epoch, epoch_num, id_, batch_num, losses.item(), np.exp(losses.item())))\n",
    "            \n",
    "    # Save the model checkpoints\n",
    "    torch.save(decoder.state_dict(), os.path.join('models', 'decoder-{}.ckpt'.format(epoch)))\n",
    "    torch.save(encoder.state_dict(), os.path.join('models', 'encoder-{}.ckpt'.format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccbe51-2a61-49b6-a8ce-92ce41d5d32b",
   "metadata": {},
   "source": [
    "save the final decode and encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2ad79e5-08b4-4c33-8259-14ff8e7fcdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), os.path.join('models', 'decoder.ckpt'))\n",
    "torch.save(encoder.state_dict(), os.path.join('models', 'encoder.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2234a-cc25-435c-9588-d3b9c9137fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
