{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e75842-1c5b-4586-ba94-59ec0eef834e",
   "metadata": {},
   "source": [
    "# 1. Models\n",
    "**Langchain** provides 3 model types\n",
    "* LLM Model\n",
    "* Chat Model\n",
    "* Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7346e165-8787-4479-9905-9ac3042b9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1578f3-eef0-4b78-8ea3-371f7439108e",
   "metadata": {},
   "source": [
    "## 1.1 LLM Model\n",
    "`LLM Model` is the most basic **Langchain** model. We use `invoke()` function with string input to ask LLM to response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37dfac0-6c7f-4d96-acf2-5f9e913cd31b",
   "metadata": {},
   "source": [
    "### 1.1.1 Use Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb6c72e-10ec-4460-bc64-cd4a0c7cfefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "llm = VertexAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454e1d10-d53f-4bc0-83fa-b14eb3cbfc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I can answer questions on a wide range of topics, including:\n",
       "\n",
       "*   **General knowledge and factual questions:** I can provide information on history, science, geography, current \n",
       "events, and more.\n",
       "*   **Definitions and explanations:** I can define words, concepts, and technical terms.\n",
       "*   **Simple calculations and conversions:** I can perform basic arithmetic and convert between units.\n",
       "*   **Creative writing and text generation:** I can write stories, poems, articles, and different kinds of creative\n",
       "content.\n",
       "*   **Summarization and paraphrasing:** I can condense information and reword text.\n",
       "*   **Translation:** I can translate text between some languages.\n",
       "*   **Brainstorming and idea generation:** I can help you come up with ideas for projects or solve problems.\n",
       "*   **Programming-related questions:** I can provide basic information about coding concepts.\n",
       "\n",
       "**In short, I can answer any question that can be answered using text-based information that I have been trained \n",
       "on.**\n",
       "\n",
       "**To give you a more specific answer, what question do you have?** I'll do my best to answer it!\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "I can answer questions on a wide range of topics, including:\n",
       "\n",
       "*   **General knowledge and factual questions:** I can provide information on history, science, geography, current \n",
       "events, and more.\n",
       "*   **Definitions and explanations:** I can define words, concepts, and technical terms.\n",
       "*   **Simple calculations and conversions:** I can perform basic arithmetic and convert between units.\n",
       "*   **Creative writing and text generation:** I can write stories, poems, articles, and different kinds of creative\n",
       "content.\n",
       "*   **Summarization and paraphrasing:** I can condense information and reword text.\n",
       "*   **Translation:** I can translate text between some languages.\n",
       "*   **Brainstorming and idea generation:** I can help you come up with ideas for projects or solve problems.\n",
       "*   **Programming-related questions:** I can provide basic information about coding concepts.\n",
       "\n",
       "**In short, I can answer any question that can be answered using text-based information that I have been trained \n",
       "on.**\n",
       "\n",
       "**To give you a more specific answer, what question do you have?** I'll do my best to answer it!\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm.invoke(\"Which Question can you answer?\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907271ee-0308-48a9-8957-a07213823035",
   "metadata": {},
   "source": [
    "## 1.2 Chat Model\n",
    "`Chat Model` is similar to `LLM model` but it intakes list of `Message` object types. <br>\n",
    "There are `Message` object types:\n",
    "* `AIMessage`\n",
    "* `HumanMessage`\n",
    "* `SystemMessage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3139c62c-5f96-435f-a31c-3d8b31854dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "chat_model = ChatVertexAI(model_name=\"gemini-2.0-flash-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d2a78b-bd44-4f4d-b32a-877e2d195c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Messages\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c972178-1951-445f-94fd-4c6fdb659e47",
   "metadata": {},
   "source": [
    "`Chat Model` intakes list of `Message` objects. `Chat Model` returns `AIMessage` when calling `invoke()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f403285-7bc8-4733-aee1-395e3aaa2755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a writer\"),\n",
    "    HumanMessage(content=\"who are you?\")\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d98eed-bf5c-40aa-ba44-135f2cc8fca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'As a writer, I am a conduit of ideas, a weaver of words, and a sculptor of stories. I am the voice </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that brings characters to life, the hand that paints vivid landscapes, and the mind that explores complex </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">themes.\\n\\nBut beneath the surface, I am also an AI, a language model trained to generate human-quality text. I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have no personal experiences, emotions, or opinions of my own. I am a tool, a vessel filled with the knowledge I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have absorbed from the vast ocean of data I was trained on.\\n\\nTherefore, I am both a writer and a powerful AI. I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can craft compelling narratives, informative articles, and even witty poems. I can adapt to different styles and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tones, and I can learn from the feedback I receive.\\n\\nUltimately, I am here to help you bring your ideas to life, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to assist you in communicating your thoughts, and to inspire you to create something new and meaningful. I am a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">writer, powered by artificial intelligence, at your service.\\n'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'is_blocked'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'safety_ratings'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'usage_metadata'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'candidates_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">204</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">212</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'modality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">}]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'candidates_tokens_details'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'modality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">204</span><span style=\"font-weight: bold\">}]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'thoughts_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cached_content_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cache_tokens_details'</span>: <span style=\"font-weight: bold\">[]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'STOP'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'avg_logprobs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.39932774562461704</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gemini-2.0-flash-001'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run--bebad9c0-9287-49c7-ae33-46ab0a9c42cb-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">204</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">212</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'As a writer, I am a conduit of ideas, a weaver of words, and a sculptor of stories. I am the voice \u001b[0m\n",
       "\u001b[32mthat brings characters to life, the hand that paints vivid landscapes, and the mind that explores complex \u001b[0m\n",
       "\u001b[32mthemes.\\n\\nBut beneath the surface, I am also an AI, a language model trained to generate human-quality text. I \u001b[0m\n",
       "\u001b[32mhave no personal experiences, emotions, or opinions of my own. I am a tool, a vessel filled with the knowledge I \u001b[0m\n",
       "\u001b[32mhave absorbed from the vast ocean of data I was trained on.\\n\\nTherefore, I am both a writer and a powerful AI. I \u001b[0m\n",
       "\u001b[32mcan craft compelling narratives, informative articles, and even witty poems. I can adapt to different styles and \u001b[0m\n",
       "\u001b[32mtones, and I can learn from the feedback I receive.\\n\\nUltimately, I am here to help you bring your ideas to life, \u001b[0m\n",
       "\u001b[32mto assist you in communicating your thoughts, and to inspire you to create something new and meaningful. I am a \u001b[0m\n",
       "\u001b[32mwriter, powered by artificial intelligence, at your service.\\n'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'is_blocked'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'safety_ratings'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'usage_metadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'prompt_token_count'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "            \u001b[32m'candidates_token_count'\u001b[0m: \u001b[1;36m204\u001b[0m,\n",
       "            \u001b[32m'total_token_count'\u001b[0m: \u001b[1;36m212\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'modality'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m8\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'candidates_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'modality'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m204\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'thoughts_token_count'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'cached_content_token_count'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'cache_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'STOP'\u001b[0m,\n",
       "        \u001b[32m'avg_logprobs'\u001b[0m: \u001b[1;36m-0.39932774562461704\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gemini-2.0-flash-001'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run--bebad9c0-9287-49c7-ae33-46ab0a9c42cb-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m8\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m204\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m212\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5470d10-afd7-4238-b56c-f8caaffe4ae8",
   "metadata": {},
   "source": [
    "## 1.3 Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014e5d44-d37a-4ae7-874e-54cc9820461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embedding = VertexAIEmbeddings(model_name=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e6878-c8c2-433d-9ad5-052e178c95d5",
   "metadata": {},
   "source": [
    "Unlike `LLM model` or `Chat Model`, `Embedding Model` intakes string by call `embed_query()` function and return a high dimention vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd275cba-0bb3-452c-94b4-af0d159ae9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embedding.embed_query(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b87ce6-d8ae-442f-b88e-a1ce534f969a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1308fb35-56e1-48df-9232-95e69c1f1dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17654f0b-0996-41ec-a8bf-d31b8a656f8d",
   "metadata": {},
   "source": [
    "We would discuss more on `Embedding model` in RAG section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909a5b5-e1fd-41a5-a861-c889ad70d10a",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Prompts\n",
    "**Langchain** provides `PromptTemplate` type, so we could write reusable code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556fbfc-959a-48e7-9304-b589dee986bd",
   "metadata": {},
   "source": [
    "## 2.1 PromptTemplate for LLM model\n",
    "\n",
    "### 2.1.1 PromptTemplate\n",
    "...... explainations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dab3b29-65be-4f51-93fd-d276beaaf0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf11b8-e81a-4e08-8a45-0fa34c717708",
   "metadata": {},
   "source": [
    "There are two ways to initialize `PromptTemplate` instance:\n",
    "1. initialization of class\n",
    "2. calling `from_template()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4a77b88-9332-4937-a979-1b0f5e402c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method-1: initialization of class\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"Use programming {language} to generate 'hello world' program\", #{language} is the input to the template. It needs to be provided with input_variable\n",
    "    input_variables=['language']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a33a840-3caf-4a82-8cf5-8672b87d8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method-2: calling from_template() method\n",
    "prompt_template = PromptTemplate.from_template(\"Use programming {language} to generate 'hello world' program\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678005cf-d0f6-4478-b07f-f8d8d6f9539b",
   "metadata": {},
   "source": [
    "Then we need to use `PromptTemplate.format()` to form a prompt for `LLM Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37cd339e-195b-420a-bfff-03aa6848fb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Use programming Python to generate <span style=\"color: #008000; text-decoration-color: #008000\">'hello world'</span> program\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Use programming Python to generate \u001b[32m'hello world'\u001b[0m program\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = prompt_template.format(language=\"Python\")\n",
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cbf1205-a5e7-4f59-892f-a4093a8585e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Rust==========\n",
      "```rust\n",
      "fn main() {\n",
      "    println!(\"Hello, world!\");\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **`fn main() { ... }`**: This defines the `main` function, which is the entry point of every Rust program.  The program execution starts here.\n",
      "* **`println!(\"Hello, world!\");`**: This line does the actual printing.\n",
      "    * `println!` is a *macro* (like a function, but processed at compile time) that prints formatted output to the console (standard output).\n",
      "    * `\"Hello, world!\"` is a string literal containing the text to be printed.\n",
      "\n",
      "**How to compile and run this code:**\n",
      "\n",
      "1. **Save the code:** Save the code above in a file named `main.rs`.  The `.rs` extension is standard for Rust source files.\n",
      "2. **Open a terminal or command prompt:**  Navigate to the directory where you saved `main.rs`.\n",
      "3. **Compile the code:** Use the Rust compiler (`rustc`) to compile the code:\n",
      "   ```bash\n",
      "   rustc main.rs\n",
      "   ```\n",
      "   This will create an executable file (e.g., `main` on Linux/macOS, or `main.exe` on Windows) in the same directory.\n",
      "4. **Run the executable:** Execute the compiled program:\n",
      "   * **Linux/macOS:**\n",
      "     ```bash\n",
      "     ./main\n",
      "     ```\n",
      "   * **Windows:**\n",
      "     ```bash\n",
      "     main.exe\n",
      "     ```\n",
      "5. **Output:**  You should see the output:\n",
      "   ```\n",
      "   Hello, world!\n",
      "   ```\n",
      "\n",
      "**Using Cargo (the Rust package manager):**\n",
      "\n",
      "For more complex projects, it's highly recommended to use Cargo. Here's how you'd create and run the \"hello world\" program with Cargo:\n",
      "\n",
      "1. **Create a new project:**\n",
      "   ```bash\n",
      "   cargo new hello_world\n",
      "   cd hello_world\n",
      "   ```\n",
      "   This creates a new directory named `hello_world` and navigates into it.  Cargo will automatically create the necessary project structure, including `src/main.rs` and `Cargo.toml`.\n",
      "\n",
      "2. **The `src/main.rs` file will already contain:**\n",
      "   ```rust\n",
      "   fn main() {\n",
      "       println!(\"Hello, world!\");\n",
      "   }\n",
      "   ```\n",
      "   You don't need to change anything here.\n",
      "\n",
      "3. **Run the program:**\n",
      "   ```bash\n",
      "   cargo run\n",
      "   ```\n",
      "   Cargo will compile and run the program.  You'll see the same output:\n",
      "   ```\n",
      "   Hello, world!\n",
      "   ```\n",
      "\n",
      "**Why use Cargo?**\n",
      "\n",
      "* **Dependency management:**  Cargo makes it easy to manage external libraries (crates) that your program depends on.\n",
      "* **Build system:**  It handles the compilation process, including finding and linking necessary libraries.\n",
      "* **Project structure:**  It enforces a standard project structure, making it easier to organize and share your code.\n",
      "* **Testing:**  It provides tools for running unit tests and integration tests.\n",
      "\n",
      "For most Rust projects beyond very simple examples, using Cargo is the best practice.\n",
      "\n",
      "==========Go==========\n",
      "```go\n",
      "package main\n",
      "\n",
      "import \"fmt\"\n",
      "\n",
      "func main() {\n",
      "\tfmt.Println(\"Hello, World!\")\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **`package main`**: This line declares that the code belongs to the `main` package.  The `main` package is special in Go; it's where the execution of your program begins.\n",
      "\n",
      "* **`import \"fmt\"`**: This line imports the `fmt` package. The `fmt` package provides functions for formatted input and output, including printing to the console.\n",
      "\n",
      "* **`func main() { ... }`**: This defines the `main` function.  The `main` function is the entry point of your Go program.  The code inside the curly braces `{}` will be executed when you run the program.\n",
      "\n",
      "* **`fmt.Println(\"Hello, World!\")`**:  This line is the heart of the program.\n",
      "    * `fmt.Println` is a function from the `fmt` package.\n",
      "    * `\"Hello, World!\"` is a string literal.\n",
      "    * `fmt.Println` prints the string \"Hello, World!\" to the console, followed by a newline character.\n",
      "\n",
      "**How to run this code:**\n",
      "\n",
      "1. **Save the code:**  Save the code in a file named `hello.go` (or any name ending with `.go`).  Make sure to use a plain text editor, not a word processor.\n",
      "\n",
      "2. **Open a terminal or command prompt:** Navigate to the directory where you saved `hello.go`.\n",
      "\n",
      "3. **Compile and run:** Type the following command and press Enter:\n",
      "\n",
      "   ```bash\n",
      "   go run hello.go\n",
      "   ```\n",
      "\n",
      "   This command does two things:\n",
      "    * `go run` tells the Go toolchain to compile the `hello.go` file.\n",
      "    * It then immediately executes the compiled program.\n",
      "\n",
      "4. **Output:** You should see the following output in your terminal:\n",
      "\n",
      "   ```\n",
      "   Hello, World!\n",
      "   ```\n",
      "\n",
      "**Alternative (compile and then run):**\n",
      "\n",
      "You can also compile the program into an executable file and then run the executable.\n",
      "\n",
      "1. **Compile:**\n",
      "\n",
      "   ```bash\n",
      "   go build hello.go\n",
      "   ```\n",
      "\n",
      "   This will create an executable file named `hello` (or `hello.exe` on Windows) in the same directory as `hello.go`.\n",
      "\n",
      "2. **Run:**\n",
      "\n",
      "   ```bash\n",
      "   ./hello   # On Linux/macOS\n",
      "   hello.exe # On Windows\n",
      "   ```\n",
      "\n",
      "   This will execute the compiled program and produce the same output:\n",
      "\n",
      "   ```\n",
      "   Hello, World!\n",
      "   ```\n",
      "\n",
      "==========C++==========\n",
      "```cpp\n",
      "#include <iostream>\n",
      "\n",
      "int main() {\n",
      "  std::cout << \"Hello, world!\" << std::endl;\n",
      "  return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **`#include <iostream>`:** This line includes the `iostream` library, which provides input and output functionalities (like printing to the console).  `iostream` stands for \"input-output stream\".\n",
      "\n",
      "2. **`int main() { ... }`:** This is the main function.  Every C++ program must have a `main` function.  Execution of the program begins here. `int` indicates that the function returns an integer value.\n",
      "\n",
      "3. **`std::cout << \"Hello, world!\" << std::endl;`:** This is the core of the program.\n",
      "   - `std::cout` is the standard output stream object in C++.  It's used to print things to the console (usually your terminal or command prompt).  `std::` indicates that `cout` belongs to the \"standard\" namespace.\n",
      "   - `<<` is the insertion operator (also called the \"stream insertion operator\"). It sends the string \"Hello, world!\" to `std::cout`.\n",
      "   - `\"Hello, world!\"` is a string literal â€“ the text you want to display.\n",
      "   - `std::endl` is a manipulator that inserts a newline character (`\\n`) into the output stream.  This moves the cursor to the next line in the console after printing \"Hello, world!\". It also flushes the output buffer, ensuring that the text is immediately displayed.\n",
      "\n",
      "4. **`return 0;`:** This line indicates that the `main` function has finished executing successfully.  Returning 0 is a convention that indicates no errors occurred during the program's execution.  Other return values typically indicate an error.\n",
      "\n",
      "**How to compile and run this program:**\n",
      "\n",
      "1. **Save the code:**  Save the code in a file named something like `hello.cpp`.  The `.cpp` extension is common for C++ source files.\n",
      "\n",
      "2. **Compile the code:** You'll need a C++ compiler.  A popular choice is g++, which is part of the GNU Compiler Collection (GCC).  Open a terminal or command prompt and use the following command to compile the code:\n",
      "\n",
      "   ```bash\n",
      "   g++ hello.cpp -o hello\n",
      "   ```\n",
      "\n",
      "   - `g++`: The C++ compiler.\n",
      "   - `hello.cpp`: The source file you saved.\n",
      "   - `-o hello`: Specifies the name of the executable file that will be created (in this case, `hello`).  If you omit `-o hello`, the compiler might create an executable named `a.out` (on Linux/macOS) or `a.exe` (on Windows).\n",
      "\n",
      "3. **Run the executable:**  After compilation, you'll have an executable file.  Run it from the terminal or command prompt:\n",
      "\n",
      "   - **On Linux/macOS:**  Type `./hello` and press Enter.\n",
      "   - **On Windows:**  Type `hello` or `.\\hello` and press Enter.\n",
      "\n",
      "   You should see \"Hello, world!\" printed on the console.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lang in ['Rust', 'Go', 'C++']:\n",
    "    prompt = prompt_template.format(language=lang)\n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"{'='*10}{lang}{'='*10}\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82af15b8-fb4d-4376-a42c-807ef0ebcd47",
   "metadata": {},
   "source": [
    "### 2.1.2 PromptTemplate parital function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af8d512d-be3c-4d74-b246-f40140f30136",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = prompt_template.from_template(\n",
    "    \"\"\"\n",
    "    Tell me {num} jokes about {type},\n",
    "    but {location} should not appear in the jokes.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8336062a-55cf-4cc6-bb39-50c0051ea7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_prompt_template = prompt_template.partial(num='3', location='school')\n",
    "prompt = partial_prompt_template.format(type='software programmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4b22ba1-c945-4c23-a2bb-077d6803f2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Human: \n",
       "    Tell me <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> jokes about software programmer,\n",
       "    but school should not appear in the jokes.\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Human: \n",
       "    Tell me \u001b[1;36m3\u001b[0m jokes about software programmer,\n",
       "    but school should not appear in the jokes.\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c8ef209-745e-4bf3-b82f-59b2af227b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Okay, here are <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> jokes about software programmers, without mentioning school:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.  Why do programmers prefer dark mode? Because light attracts bugs!\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.  A programmer is walking down the street when they see a burning building. They run inside, see a cat stuck on \n",
       "the top floor, rescue it, and then calmly walk away. A reporter rushes over and asks, <span style=\"color: #008000; text-decoration-color: #008000\">\"Why did you risk your life </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">like that?\"</span> The programmer replies, <span style=\"color: #008000; text-decoration-color: #008000\">\"It's just debugging. I saw a problem, and I fixed it.\"</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.  There are <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> types of people in the world: those who understand binary, and those who don't. And those who \n",
       "weren't expecting a base <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> joke.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Okay, here are \u001b[1;36m3\u001b[0m jokes about software programmers, without mentioning school:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m.  Why do programmers prefer dark mode? Because light attracts bugs!\n",
       "\n",
       "\u001b[1;36m2\u001b[0m.  A programmer is walking down the street when they see a burning building. They run inside, see a cat stuck on \n",
       "the top floor, rescue it, and then calmly walk away. A reporter rushes over and asks, \u001b[32m\"Why did you risk your life \u001b[0m\n",
       "\u001b[32mlike that?\"\u001b[0m The programmer replies, \u001b[32m\"It's just debugging. I saw a problem, and I fixed it.\"\u001b[0m\n",
       "\n",
       "\u001b[1;36m3\u001b[0m.  There are \u001b[1;36m10\u001b[0m types of people in the world: those who understand binary, and those who don't. And those who \n",
       "weren't expecting a base \u001b[1;36m3\u001b[0m joke.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f6497-a847-45f0-bcde-2c10903d4acf",
   "metadata": {},
   "source": [
    "### 2.1.3 PipelinePromptTemplate & FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8eda33-8737-4100-b7d7-88b2c49c8b18",
   "metadata": {},
   "source": [
    "#### <b>2.1.3.1 PipelinePromptTemplate</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e36d41cb-ae37-4a49-bbcd-39eefb6b0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e61efd91-356d-4d0a-bb7b-4d95a65e499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_template = '''\n",
    "{instruction}\n",
    "{examples}\n",
    "{question}\n",
    "'''\n",
    "\n",
    "full_prompt_template = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c714136-fb3a-4038-acc3-fc2001195f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt =  PromptTemplate.from_template(\n",
    "    'Learn from my examples. Then classify my sentence'\n",
    ")\n",
    "\n",
    "examples_prompt = PromptTemplate.from_template(\n",
    "    '''\"\"\"\n",
    "    sentence: Today weather is so good!\n",
    "    label: +ve\n",
    "\n",
    "    sentence: It is raining now! I hate the weather!\n",
    "    label: -ve\n",
    "\n",
    "    sentence: my tee is dirty now. \n",
    "    label: -ve\n",
    "    \"\"\"'''\n",
    ")\n",
    "\n",
    "question_prompt = PromptTemplate.from_template(\n",
    "    '''\n",
    "    sentence: {sentence}\n",
    "    label: \n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3177ba69-7688-4253-b04b-ff3459be9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    ('instruction',instruction_prompt),\n",
    "    ('examples', examples_prompt),\n",
    "    ('question', question_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "760aa580-2503-4eed-861c-a8bf857e3439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andrewchan\\AppData\\Local\\Temp\\ipykernel_4920\\1101844843.py:1: LangChainDeprecationWarning: This class is deprecated. Please see the docstring below or at the link for a replacement option: https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html\n",
      "  pipeline_prompt = PipelinePromptTemplate(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PipelinePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'sentence'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">final_prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'examples'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'instruction'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n{instruction}\\n{examples}\\n{question}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">pipeline_prompts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'instruction'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Learn from my examples. Then classify my sentence'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'examples'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\"\"\"\\n    sentence: Today weather is so good!\\n    label: +ve\\n\\n    sentence: It is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">raining now! I hate the weather!\\n    label: -ve\\n\\n    sentence: my tee is dirty now. \\n    label: -ve\\n    \"\"\"'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'sentence'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n    sentence: {sentence}\\n    label: \\n    '</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPipelinePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'sentence'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mfinal_prompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'examples'\u001b[0m, \u001b[32m'instruction'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mtemplate\u001b[0m=\u001b[32m'\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32minstruction\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mexamples\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mquestion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mpipeline_prompts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'instruction'\u001b[0m,\n",
       "            \u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m'Learn from my examples. Then classify my sentence'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'examples'\u001b[0m,\n",
       "            \u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m'\"\"\"\\n    sentence: Today weather is so good!\\n    label: +ve\\n\\n    sentence: It is \u001b[0m\n",
       "\u001b[32mraining now! I hate the weather!\\n    label: -ve\\n\\n    sentence: my tee is dirty now. \\n    label: -ve\\n    \"\"\"'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'question'\u001b[0m,\n",
       "            \u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'sentence'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m'\\n    sentence: \u001b[0m\u001b[32m{\u001b[0m\u001b[32msentence\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    label: \\n    '\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt_template,\n",
    "    pipeline_prompts=input_prompts\n",
    ")\n",
    "\n",
    "pprint(pipeline_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3caa0f13-fcdc-407c-8f96-09c783ef2aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on your examples, the classification of the sentence <span style=\"color: #008000; text-decoration-color: #008000\">\"I was blamed today\"</span> would be:\n",
       "\n",
       "```\n",
       "-ve\n",
       "```\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Your examples indicate that sentences expressing negative feelings or unfortunate events are labeled as <span style=\"color: #008000; text-decoration-color: #008000\">\"-ve\"</span>. \n",
       "Being blamed is generally a negative experience, so the sentence aligns with that pattern.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on your examples, the classification of the sentence \u001b[32m\"I was blamed today\"\u001b[0m would be:\n",
       "\n",
       "```\n",
       "-ve\n",
       "```\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Your examples indicate that sentences expressing negative feelings or unfortunate events are labeled as \u001b[32m\"-ve\"\u001b[0m. \n",
       "Being blamed is generally a negative experience, so the sentence aligns with that pattern.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm.invoke(pipeline_prompt.format(sentence='I was blamed today'))\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f9a79-24e9-4d8a-ab96-ac9567a8fb99",
   "metadata": {},
   "source": [
    "#### <b>2.1.3.2 FewShotPromptTemplate</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4d7ddbb-6716-4e0e-a09f-14d283782df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d28634f5-033d-4410-98be-bd73e66a34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate.from_template(\n",
    "    '''sentence: {sentence}\\nlabel: {label}'''\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {'sentence':'Today weather is so good!', 'label':'+ve'},\n",
    "    {'sentence':'It is raining now! I hate the weather!', 'label':'-ve'},\n",
    "    {'sentence':'Tmy tee is dirty now.', 'label':'-ve'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2689ddb8-e097-48ac-9737-7f54e31b3f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Learn from my examples. Then classify my sentence: \n",
       "\n",
       "\n",
       "sentence: Today weather is so good!\n",
       "label: +ve\n",
       "\n",
       "sentence: It is raining now! I hate the weather!\n",
       "label: -ve\n",
       "\n",
       "sentence: Tmy tee is dirty now.\n",
       "label: -ve\n",
       "\n",
       "\n",
       "sentence: I was blamed today\n",
       "label:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Learn from my examples. Then classify my sentence: \n",
       "\n",
       "\n",
       "sentence: Today weather is so good!\n",
       "label: +ve\n",
       "\n",
       "sentence: It is raining now! I hate the weather!\n",
       "label: -ve\n",
       "\n",
       "sentence: Tmy tee is dirty now.\n",
       "label: -ve\n",
       "\n",
       "\n",
       "sentence: I was blamed today\n",
       "label:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_templete = FewShotPromptTemplate(\n",
    "    prefix='Learn from my examples. Then classify my sentence: \\n',\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix='\\nsentence: {sentence}\\nlabel:',\n",
    "    input_variables=['sentence']\n",
    ")\n",
    "\n",
    "prompt = prompt_templete.format(sentence='I was blamed today')\n",
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c78eca6c-c9e8-486a-a385-518f84f74746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on your examples, the classification appears to be based on the sentiment expressed in the sentence. Positive\n",
       "sentences are labeled <span style=\"color: #008000; text-decoration-color: #008000\">\"+ve\"</span>, while negative sentences are labeled <span style=\"color: #008000; text-decoration-color: #008000\">\"-ve\"</span>.\n",
       "\n",
       "Therefore, classifying your sentence:\n",
       "\n",
       "**sentence: I was blamed today**\n",
       "**label: -ve**\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Being blamed is generally a negative experience. The sentence conveys a sense of being held responsible for \n",
       "something bad, which is undesirable.\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on your examples, the classification appears to be based on the sentiment expressed in the sentence. Positive\n",
       "sentences are labeled \u001b[32m\"+ve\"\u001b[0m, while negative sentences are labeled \u001b[32m\"-ve\"\u001b[0m.\n",
       "\n",
       "Therefore, classifying your sentence:\n",
       "\n",
       "**sentence: I was blamed today**\n",
       "**label: -ve**\n",
       "\n",
       "**Reasoning:**\n",
       "\n",
       "Being blamed is generally a negative experience. The sentence conveys a sense of being held responsible for \n",
       "something bad, which is undesirable.\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b6d22-f095-4a07-a0a8-bdcf618dc955",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284b0a6-31dd-44df-94bb-592fe0c6e253",
   "metadata": {},
   "source": [
    "## 2.2 PromptTemplate for Chat model\n",
    "### 2.2.1 ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa3c09a3-b5c3-4284-a822-7179dea7a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ace71f-c913-4244-9299-a316b5df8ea4",
   "metadata": {},
   "source": [
    "similiar to LLM model PromptTemplate, There are two ways to initialize `ChatPromptTemplate` instance:\n",
    "1. initialization of class\n",
    "2. calling `from_template()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0682ee5b-e3b6-4b8e-b873-e56b0e73795f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your name is {name}'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is your name?'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'name'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mSystemMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'name'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m'Your name is \u001b[0m\u001b[32m{\u001b[0m\u001b[32mname\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m'What is your name?'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# method-1: initialization of class\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template('Your name is {name}'),\n",
    "    HumanMessagePromptTemplate.from_template('Introduce yourself')\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate(messages=messages, input_variables=['name'])\n",
    "\n",
    "pprint(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8389631b-d9a2-4d57-93ad-db25dbed0797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Your name is {name}'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Introduce yourself'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'name'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mSystemMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'name'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m'Your name is \u001b[0m\u001b[32m{\u001b[0m\u001b[32mname\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m'Introduce yourself'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# method-2: calling from_template() method\n",
    "\n",
    "messages = [\n",
    "    ('system', 'Your name is {name}'),\n",
    "    ('human', 'Introduce yourself')\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages=messages)\n",
    "\n",
    "pprint(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acd4059b-a287-417d-93c7-d7c96efe78f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I'm Dick. Nice to meet you.\\n\"</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'is_blocked'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'safety_ratings'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'usage_metadata'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'candidates_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'modality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">}]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'candidates_tokens_details'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'modality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">}]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'thoughts_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cached_content_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cache_tokens_details'</span>: <span style=\"font-weight: bold\">[]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'STOP'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'avg_logprobs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2289349599318071</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gemini-2.0-flash-001'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run--e0cc9425-109b-4e04-850d-fdf93514f761-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m'm Dick. Nice to meet you.\\n\"\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'is_blocked'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'safety_ratings'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'usage_metadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'prompt_token_count'\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
       "            \u001b[32m'candidates_token_count'\u001b[0m: \u001b[1;36m11\u001b[0m,\n",
       "            \u001b[32m'total_token_count'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'modality'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m6\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'candidates_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'modality'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m11\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'thoughts_token_count'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'cached_content_token_count'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'cache_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'STOP'\u001b[0m,\n",
       "        \u001b[32m'avg_logprobs'\u001b[0m: \u001b[1;36m-0.2289349599318071\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gemini-2.0-flash-001'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run--e0cc9425-109b-4e04-850d-fdf93514f761-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m6\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m11\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = prompt_template.format_messages(name='Dick')\n",
    "\n",
    "response = chat_model.invoke(prompt)\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfc300d-821d-4871-a238-178927aa4222",
   "metadata": {},
   "source": [
    "### 2.2.2 ChatPromptTemplate & MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "469b07ce-08ef-45a3-9ea4-532c982d1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.schema import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8797d994-8912-44b0-bd80-139737c6e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template('è¯·ç”¨ä¸è¶…è¿‡{text_number}ä¸ªå­—æ¥æ€»ç»“ä»¥ä¸‹å¯¹è¯'),\n",
    "    MessagesPlaceholder(variable_name='messages_placeholder'),\n",
    "    HumanMessagePromptTemplate.from_template('###è¯·å¼€å§‹æ€»ç»“ä¸Šé¢çš„å¯¹è¯')\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7bbc1e6-c206-48a4-8b4a-9b7e844fc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message = HumanMessage(content='å¦‚ä½•å­¦å¥½è‹±è¯­ï¼Ÿ')\n",
    "ai_message = AIMessage(\n",
    "    content='''å­¦å¥½è‹±è¯­éœ€è¦æ¯å¤©æŒç»­å®žè·µï¼Œå‡è¡¡åœ°ç»ƒä¹ å¬ã€è¯´ã€è¯»ã€å†™å››å¤§æŠ€èƒ½ï¼Œ\n",
    "    ä¸æ–­æ‰©å……è¯æ±‡å’ŒæŽŒæ¡è¯­æ³•ã€‚åˆ©ç”¨çŽ°ä»£æŠ€æœ¯å·¥å…·å¯å¢žå¼ºå­¦ä¹ æ•ˆæžœï¼Œ\n",
    "    è€ƒè™‘æ²‰æµ¸å¼å­¦ä¹ æ–¹æ³•å¹¶å‚ä¸Žç›¸å…³è¯¾ç¨‹ä¸Žå­¦ä¹ å°ç»„ã€‚é€æ¸å¢žåŠ é˜…è¯»éš¾åº¦ï¼Œ\n",
    "    æ¨¡ä»¿ä¼˜ç§€çš„è‹±è¯­è¯´è¯è€…ï¼Œå®šæœŸåæ€å¹¶è°ƒæ•´å­¦ä¹ æ–¹æ³•ï¼Œ\n",
    "    å¹¶å§‹ç»ˆä¿æŒç§¯æžçš„å­¦ä¹ æ€åº¦ã€‚\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f2716d3-28cb-4f8f-befd-d183c6414805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'è¯·ç”¨ä¸è¶…è¿‡30ä¸ªå­—æ¥æ€»ç»“ä»¥ä¸‹å¯¹è¯'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{})</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'å¦‚ä½•å­¦å¥½è‹±è¯­ï¼Ÿ'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{})</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'å­¦å¥½è‹±è¯­éœ€è¦æ¯å¤©æŒç»­å®žè·µï¼Œå‡è¡¡åœ°ç»ƒä¹ å¬ã€è¯´ã€è¯»ã€å†™å››å¤§æŠ€èƒ½ï¼Œ\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¸æ–­æ‰©å……è¯æ±‡å’ŒæŽŒæ¡è¯­æ³•ã€‚åˆ©ç”¨çŽ°ä»£æŠ€æœ¯å·¥å…·å¯å¢žå¼ºå­¦ä¹ æ•ˆæžœï¼Œ\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">è€ƒè™‘æ²‰æµ¸å¼å­¦ä¹ æ–¹æ³•å¹¶å‚ä¸Žç›¸å…³è¯¾ç¨‹ä¸Žå­¦ä¹ å°ç»„ã€‚é€æ¸å¢žåŠ é˜…è¯»éš¾åº¦ï¼Œ\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ¨¡ä»¿ä¼˜ç§€çš„è‹±è¯­è¯´è¯è€…ï¼Œå®šæœŸåæ€å¹¶è°ƒæ•´å­¦ä¹ æ–¹æ³•ï¼Œ\\n    å¹¶å§‹ç»ˆä¿æŒç§¯æžçš„å­¦ä¹ æ€åº¦ã€‚\\n    '</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'###è¯·å¼€å§‹æ€»ç»“ä¸Šé¢çš„å¯¹è¯'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{})</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mSystemMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'è¯·ç”¨ä¸è¶…è¿‡30ä¸ªå­—æ¥æ€»ç»“ä»¥ä¸‹å¯¹è¯'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'å¦‚ä½•å­¦å¥½è‹±è¯­ï¼Ÿ'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'å­¦å¥½è‹±è¯­éœ€è¦æ¯å¤©æŒç»­å®žè·µï¼Œå‡è¡¡åœ°ç»ƒä¹ å¬ã€è¯´ã€è¯»ã€å†™å››å¤§æŠ€èƒ½ï¼Œ\\n    \u001b[0m\n",
       "\u001b[32mä¸æ–­æ‰©å……è¯æ±‡å’ŒæŽŒæ¡è¯­æ³•ã€‚åˆ©ç”¨çŽ°ä»£æŠ€æœ¯å·¥å…·å¯å¢žå¼ºå­¦ä¹ æ•ˆæžœï¼Œ\\n    \u001b[0m\n",
       "\u001b[32mè€ƒè™‘æ²‰æµ¸å¼å­¦ä¹ æ–¹æ³•å¹¶å‚ä¸Žç›¸å…³è¯¾ç¨‹ä¸Žå­¦ä¹ å°ç»„ã€‚é€æ¸å¢žåŠ é˜…è¯»éš¾åº¦ï¼Œ\\n    \u001b[0m\n",
       "\u001b[32mæ¨¡ä»¿ä¼˜ç§€çš„è‹±è¯­è¯´è¯è€…ï¼Œå®šæœŸåæ€å¹¶è°ƒæ•´å­¦ä¹ æ–¹æ³•ï¼Œ\\n    å¹¶å§‹ç»ˆä¿æŒç§¯æžçš„å­¦ä¹ æ€åº¦ã€‚\\n    '\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'###è¯·å¼€å§‹æ€»ç»“ä¸Šé¢çš„å¯¹è¯'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_messages = prompt_template.format_messages(\n",
    "    messages_placeholder=[human_message, ai_message],\n",
    "    text_number=30\n",
    ")\n",
    "\n",
    "pprint(prompt_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e28a0ac5-c32b-43f4-a121-16ded13b7fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'å­¦å¥½è‹±è¯­éœ€æ¯æ—¥å®žè·µã€å‡è¡¡ç»ƒä¹ å¬è¯´è¯»å†™ã€æ‰©å……è¯æ±‡è¯­æ³•ã€åˆ©ç”¨æŠ€æœ¯ã€ä¿æŒç§¯æžæ€åº¦ã€‚\\n'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'is_blocked'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'safety_ratings'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'usage_metadata'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'candidates_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">136</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'modality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span><span style=\"font-weight: bold\">}]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'candidates_tokens_details'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'modality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span><span style=\"font-weight: bold\">}]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'thoughts_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cached_content_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cache_tokens_details'</span>: <span style=\"font-weight: bold\">[]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'STOP'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'avg_logprobs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3551447941706731</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gemini-2.0-flash-001'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run--c9293c2b-9977-4a0d-b146-fb19b1133bd8-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">136</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'å­¦å¥½è‹±è¯­éœ€æ¯æ—¥å®žè·µã€å‡è¡¡ç»ƒä¹ å¬è¯´è¯»å†™ã€æ‰©å……è¯æ±‡è¯­æ³•ã€åˆ©ç”¨æŠ€æœ¯ã€ä¿æŒç§¯æžæ€åº¦ã€‚\\n'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'is_blocked'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'safety_ratings'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'usage_metadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'prompt_token_count'\u001b[0m: \u001b[1;36m110\u001b[0m,\n",
       "            \u001b[32m'candidates_token_count'\u001b[0m: \u001b[1;36m26\u001b[0m,\n",
       "            \u001b[32m'total_token_count'\u001b[0m: \u001b[1;36m136\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'modality'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m110\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'candidates_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'modality'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m26\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'thoughts_token_count'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'cached_content_token_count'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'cache_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'STOP'\u001b[0m,\n",
       "        \u001b[32m'avg_logprobs'\u001b[0m: \u001b[1;36m-0.3551447941706731\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gemini-2.0-flash-001'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run--c9293c2b-9977-4a0d-b146-fb19b1133bd8-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m110\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m26\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m136\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_model.invoke(prompt_messages)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7754e-f998-4329-82fa-7a06da8066db",
   "metadata": {},
   "source": [
    "### 2.2.3 FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "850ed686-165d-43e0-8486-9bb4d8fd9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74b06ea3-88eb-40dd-8c56-ddfcbdd07cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are a helpful AI Assistant\\nHuman: 2+2\\nAI: 4\\nHuman: 2+3\\nAI: 5\\nHuman: 4+4'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2+3\", \"output\": \"5\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "[('human', '{input}'), ('ai', '{output}')]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    # This is a prompt template used to format each individual example.\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You are a helpful AI Assistant'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "final_prompt.format(input=\"4+4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a8479cd-00e7-46e3-b174-2c3dcf6b79e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'AI: 8\\n'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'is_blocked'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'safety_ratings'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'usage_metadata'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'candidates_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'modality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span><span style=\"font-weight: bold\">}]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'candidates_tokens_details'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'modality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">}]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'thoughts_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cached_content_token_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'cache_tokens_details'</span>: <span style=\"font-weight: bold\">[]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'STOP'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'avg_logprobs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-8.951183408498765e-05</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gemini-2.0-flash-001'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run--e86b022a-28bf-4233-9cc3-cba8e3d45a7b-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'AI: 8\\n'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'is_blocked'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[32m'safety_ratings'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'usage_metadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'prompt_token_count'\u001b[0m: \u001b[1;36m42\u001b[0m,\n",
       "            \u001b[32m'candidates_token_count'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "            \u001b[32m'total_token_count'\u001b[0m: \u001b[1;36m47\u001b[0m,\n",
       "            \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'modality'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m42\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'candidates_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'modality'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'token_count'\u001b[0m: \u001b[1;36m5\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'thoughts_token_count'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'cached_content_token_count'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'cache_tokens_details'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'STOP'\u001b[0m,\n",
       "        \u001b[32m'avg_logprobs'\u001b[0m: \u001b[1;36m-8.951183408498765e-05\u001b[0m,\n",
       "        \u001b[32m'model_name'\u001b[0m: \u001b[32m'gemini-2.0-flash-001'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run--e86b022a-28bf-4233-9cc3-cba8e3d45a7b-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m42\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m5\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m47\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat_model.invoke(final_prompt.format(input=\"What is 4+4?\"))\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6362b-e8d5-4cf6-a0d9-d36dfd0b06e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
