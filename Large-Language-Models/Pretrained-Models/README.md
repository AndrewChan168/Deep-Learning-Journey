# 1. Introduction
When we are going to fine tuning an Pre-Trained Large Language Model (LLM), we could not skip **Hugging Face**. <br>
Hugging Face provides a platform for hosting open source pretrained models and datasets. 

## 1.1 Reference Books
1. Hugging Face in action
2. Hands-On Large Language Models

# 2. Usage of Hugging Face
Hugging Face provides webpage for us to search models & datasets. Apart from webpage, Hugging Face also provides `transformers` python package for us to Fine-Tuning pretrained models to adapting our need.

## 2.1 Searching for model and datasets
On the Hugging Face webpage, there are tags to organize *models* & *datasets*. 

# 3. transformers package

## 3.1 Setup transformers package
Follow ![00-Basic-Setup](00-Basic-Setup.ipynb) for installing packages needed for fine tuning.

## 3.2 Outline of usage of transformers package
Follow ![01-Using-Transformers-for-NLP-tasks](01-Using-Transformers-for-NLP-tasks.ipynb) for the outline of usage of `transformers` package to fine-tune pretrained model.

# 4. Common tasks for Pretrained Models
We will go through the each common task for Pretrained Model by two steps
Step-1: Directly Usage the Pretrained Model
Step-2: Fine-Tune the pretrained model for better performance

## 4.1 Embedding

## 4.2 Clustering on Embedding

## 4.3 Classification on Embedding

## 4.4 Topic Models on Embedding

## 4.5 Text Generation

## 4.6 Multimodals

