{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ae9032-2a01-4f2a-87e0-f575e2406db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb135cf-706d-49cc-9133-affbc20c54df",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae4ff4-f49b-4a64-8ce1-b2fe65ea1963",
   "metadata": {},
   "source": [
    "# Fine Tune with LoRA\n",
    "In this notebook, we are going to train an instruction type pretrained model to sounds like Master Yoda in Star War series. We will adapt the LoRA and quantized model to reduce the resource usage during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e993fd-fe08-4169-af78-ceb76c4e8464",
   "metadata": {},
   "source": [
    "## Load a Quantized Base Model\n",
    "We will load a quantized model which takes up less space in GPU's RAM. A quantized model <u>replaces the original weights with approximate values that are represented by fewer bits</u>. This practice reduces the model's memory footprint.<br>\n",
    "To load quantized model, we have to fill `quantization_config` argument of `from_pretrained()` function with `BitsAndBytesConfig` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547235ec-ad01-45dc-8af0-d5703d832315",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1ac353-8376-46f2-a178-c92b3e5d3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f4ee72-0752-427c-9b01-8b2cb584e4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5164a947fc4eb4b05834581a301807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda:0\", quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19b450c-4eae-471c-bc1a-85b3927e2401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206.341504\n"
     ]
    }
   ],
   "source": [
    "# check how much space the model takes\n",
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47641398-b4e2-45f0-8314-38e09cb00dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3ForCausalLM(\n",
      "  (model): Phi3Model(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3Attention(\n",
      "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "          (activation_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): Phi3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ddcc3-595d-4bb6-a8b5-0e4f01c16f52",
   "metadata": {},
   "source": [
    "## Setup LoRA\n",
    "LoRA are attached to layers of pretrained model. This time LoRA would attached every one of the quantized layers. <br>\n",
    "In our case, the quantized layers are frozen. We only need to train LoRA layers to adapt the domain usage. <u><b>LoRA adapters</b> on a quantized model would drastically reduce the total number of trainable parameters.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bbe200-56a4-4575-b929-fea5758853a1",
   "metadata": {},
   "source": [
    "To set LoRA, we have to use `peft` package. We need three steps:\n",
    "1. call `prepare_model_for_kbit_training()` to wrapped the pretrained model\n",
    "2. Configure instance of `LoraConfig`\n",
    "3. apply the configuration with `get_peft_model()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc17bf7f-1baf-4238-9c1e-3b5a2907f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dfc1cbb-4c89-4baf-a7c7-7c456ed0ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b42639-1268-499e-ab7d-0a9b040aebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16, # multiplier, usually 2*r\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c602169e-42aa-4c86-91ab-ee479321506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.074944\n"
     ]
    }
   ],
   "source": [
    "# check how much space the model takes\n",
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b1447-7913-4f28-9c0c-538388dbedf6",
   "metadata": {},
   "source": [
    "450MB size was added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d20179f0-888d-413c-941d-4982b1f7fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Phi3ForCausalLM(\n",
      "      (model): Phi3Model(\n",
      "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x Phi3DecoderLayer(\n",
      "            (self_attn): Phi3Attention(\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (qkv_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): Phi3MLP(\n",
      "              (gate_up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=16384, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (activation_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
      "        (rotary_emb): Phi3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd02c9c-0dce-4fc2-a1a4-3204e3aa6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "### reusable function to print out trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    train_params, total_params = model.get_nb_trainable_parameters()\n",
    "    print(f\"Trainable parameters:      {train_params/1e6:.2f}MB\")\n",
    "    print(f'Total parameters:          {total_params/1e6:.2f}MB')\n",
    "    print(f'% of trainable parameters: {100*train_params/total_params:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce31e1db-d252-49c4-a3c1-6132de84bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:      12.58MB\n",
      "Total parameters:          3833.66MB\n",
      "% of trainable parameters: 0.33%\n"
     ]
    }
   ],
   "source": [
    "# check how many trainable parameters now\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bdfbe4-dc4e-4778-b024-4e7ee31ae52e",
   "metadata": {},
   "source": [
    "After applied LoRA, we have only to train 0.33% of the original parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d005d-9799-4ec5-8d8b-c7f8c9af9059",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431e8c2-9ea1-4b26-bb2a-b62ac867df47",
   "metadata": {},
   "source": [
    "### Load the dataset from Hugging Face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57020594-d6dc-4825-8b0b-62857e2f7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba3e5dbe-e0b8-409c-83a8-954814ce3e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b53d9e215264bd5b620380f3e2e9e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98050562c1e4462796edc28b51422db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentences.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4c8452f3664576a4d4f9e8f35ebb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['sentence', 'translation', 'translation_extra']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "#dataset.column_names\n",
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db9623f0-9061-455f-820e-69fdf0fa2c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'translation', 'translation_extra'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f129ddb5-4745-4f5d-a32b-d0a295bc690a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
       " 'translation': 'On the smooth planks, the birch canoe slid.',\n",
       " 'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dec0d9-f611-4681-8cb1-dd17205633ac",
   "metadata": {},
   "source": [
    "### Formating dataset for training usage\n",
    "`Phi` model has requirement on format of input data. To fulfill the requirement, we need to transform the dataset to the format it required. In this part, we prepare an higher order function to transform the format of dataset with `map()` function of dataset.<br>\n",
    "We may later call `tokenizer.apply_chat_template()` function or `tokenizer.chat_template` to explode the format of data to be feed to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c57cc9ff-6aa3-47f7-b003-8336187c8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### reusable function to \n",
    "### convert dataset from prompt/completion format \n",
    "### to conversational format\n",
    "\n",
    "def format_dataset(data):\n",
    "    if isinstance(data[\"prompt\"], list):\n",
    "        outputs = []\n",
    "        for i in range(len(data[\"prompt\"])):\n",
    "            converted_data = [\n",
    "                {\"role\":\"user\", \"content\":data[\"prompt\"][i]},\n",
    "                {\"role\":\"assistant\", \"content\":data[\"completion\"][i]}\n",
    "            ]\n",
    "            outputs.append(converted_data)\n",
    "        return {'messages': outputs}\n",
    "    else:\n",
    "        converted_data = [\n",
    "            {\"role\":\"user\", \"content\":data[\"prompt\"]},\n",
    "            {\"role\":\"assistant\", \"content\":data[\"completion\"]}\n",
    "        ]\n",
    "        return {'messages': converted_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d89e3af-1a10-4fb1-9bca-dcec7354c1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccc39cdfbc040029ffdb6eea47973ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7352347-bd60-4778-816a-d38947d72ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc327725-b5b1-46e7-b753-7e292fc86783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'The birch canoe slid on the smooth planks.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad487b-d44d-4ea7-bfa0-adf4b63d00e2",
   "metadata": {},
   "source": [
    "### Tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff427260-e588-418c-8322-a1a35fb62fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c23349-c031-4276-8979-846dea6e759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a0a1e-0e11-4c8b-bb36-c41f487ad7d2",
   "metadata": {},
   "source": [
    "configuring special token for fulfilling model's requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6785ec0-ab55-44ed-93dc-3e7462ef0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918f98b-abd6-4835-b6c9-b4a25edfe100",
   "metadata": {},
   "source": [
    "take a look on template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d365a9b-c005-4b06-a223-e36419543c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8182f3b5-d34a-49e7-a60b-b0362c12befe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'The birch canoe slid on the smooth planks.', 'role': 'user'},\n",
       " {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23a50ed4-7840-404a-9f62-7e24dc3d7662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The birch canoe slid on the smooth planks.<|end|>\n",
      "<|assistant|>\n",
      "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(dataset[0]['messages'], tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4eb9a-bf8e-4149-ad18-f84bf4a0ec7a",
   "metadata": {},
   "source": [
    "## Fine Tune with SFT\n",
    "Fine-tuning more or less follows same training procedure as training a deep learning model. We could use `Trainer` from transformers package or `SFTTrainer` from trl package instead of writting our own training loop. In my case, we use `SFTTrainer` of trl package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21831c43-961b-4ccc-801b-acdf3257ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb2cfcd0-a077-4872-bef3-7760d67d4d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    # configs for memory usage\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
    "    gradient_accumulation_steps=1,  \n",
    "    per_device_train_batch_size=16, \n",
    "    auto_find_batch_size=True,\n",
    "    # configs for dataset\n",
    "    max_length=64,\n",
    "    packing=True,\n",
    "    packing_strategy='wrapped',\n",
    "    # configs for training process\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    optim='paged_adamw_8bit',\n",
    "    # config for logging\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./phi3-mini-yoda-adapter',\n",
    "    # other configuration\n",
    "    bf16=torch.cuda.is_bf16_supported(including_emulation=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a34a36c-964f-4f0a-a72d-cafb4ae546fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5423a3b59d984efdb2e58c6d7510802c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37717a4ece3f40929c2cc2dbb06018c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "    model=model.base_model.model,\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32532837-9041-406a-a6bf-c1e7f38b5bdf",
   "metadata": {},
   "source": [
    "Take a look on how SFT-Trainer process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67d46ed4-7a07-4c6b-9468-08e0da8de395",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = sft_trainer.get_train_dataloader()\n",
    "batch = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc05223a-1b71-49a0-94aa-533e101a2e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3974, 29892,  4337,   278,   325,   271, 29892,   366,  1818, 29889,\n",
       "        32007, 32000, 32010,   450,   289,   935,   310,   278,   282,   457,\n",
       "         5447,   471,   528,  4901,   322,  6501, 29889, 32007, 32001, 26399,\n",
       "         1758,  4317, 29889,  1383,  4901,   322,  6501, 29892,   278,   289,\n",
       "          935,   310,   278,   282,   457,  5447,   471, 29889, 32007, 32000,\n",
       "        32010,   951,  5989,  2507, 17354,   322, 13328,   297,   278,  6416,\n",
       "        29889, 32007, 32001,   512], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e60bb167-9865-4a42-9634-d04a62038f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3974, 29892,  4337,   278,   325,   271, 29892,   366,  1818, 29889,\n",
       "        32007, 32000, 32010,   450,   289,   935,   310,   278,   282,   457,\n",
       "         5447,   471,   528,  4901,   322,  6501, 29889, 32007, 32001, 26399,\n",
       "         1758,  4317, 29889,  1383,  4901,   322,  6501, 29892,   278,   289,\n",
       "          935,   310,   278,   282,   457,  5447,   471, 29889, 32007, 32000,\n",
       "        32010,   951,  5989,  2507, 17354,   322, 13328,   297,   278,  6416,\n",
       "        29889, 32007, 32001,   512], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4e25c-5f2b-4fea-a391-bc379e003ef5",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ffd55c9-eff1-46e2-9f35-2be1628115a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/220 16:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.839255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.822768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.604206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.517168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.398046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.296729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.191879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.997083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.897667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.630632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.427797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.434262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.375440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.351811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.320194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.304692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.297876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.271273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.261188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.243005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.242606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=220, training_loss=0.8327989242293617, metrics={'train_runtime': 971.9506, 'train_samples_per_second': 3.508, 'train_steps_per_second': 0.226, 'total_flos': 4890970340720640.0, 'train_loss': 0.8327989242293617})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f38c1571-f4b8-4873-a1bb-10a31105aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the model\n",
    "sft_trainer.save_model('Phi-3-mini-4k-yoda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d2100-dac4-4eaa-a9b8-0e48e58d8162",
   "metadata": {},
   "source": [
    "# Test with fine tuned model\n",
    "We have trained the master Yoda speaking model. Let's compare the generated response from model before and after fine tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75649bec-eb0c-45d3-b2b5-6f02e6fae8ff",
   "metadata": {},
   "source": [
    "We will prepare two reusable functions:\n",
    "- `generate_prompt`: transform input prompt from user to the one with format that required by the model\n",
    "- `generate_response`: provide with model and tokenizer to generate response of prompt from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "947a8f84-2b5a-4726-8c2a-38672afdd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(tokenizer, sentence):\n",
    "    converted = [{\"role\":\"user\", \"content\":sentence}]\n",
    "    prompt = tokenizer.apply_chat_template(converted, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f965ee2-d626-4d5a-b236-964beffae52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "May the Force be with you!<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"May the Force be with you!\"\n",
    "prompt = generate_prompt(tokenizer, sentence)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "349d4fe2-cc49-44ea-8ff7-be8e8d2281a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
    "    tokenized = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "    model.eval()\n",
    "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
    "    with ctx:\n",
    "        gen_output = model.generate(**tokenized, eos_token_id=tokenizer.eos_token_id, max_new_tokens=max_new_tokens)\n",
    "    output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a6665fe-26be-49d7-8466-9e0583d3daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> May the Force be with you!<|end|><|assistant|> With you, may the Force be.<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate_response(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a9397e-568d-4b15-8725-378c44ff5b1e",
   "metadata": {},
   "source": [
    "It sounds like Master Yoda now. Let's compare with the model before fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8b96e24-7e10-4bd2-9b21-78302fa22066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65e6a9811414ec0b83a46803abfa425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda:0\", quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fd4d76b9-645a-46f6-952d-0f8a6084dabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|> May the Force be with you!<|end|><|assistant|> Always here to assist you. If you have any questions or need support, feel free to ask. May the Force be with you!<|end|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(generate_response(raw_model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f4dd9-f227-47ab-b4c4-077f93c450f9",
   "metadata": {},
   "source": [
    "What a huge difference!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2218585-e9f1-435e-945c-3878eda1c28e",
   "metadata": {},
   "source": [
    "## Further reduce the memory usage\n",
    "We could further reduce the memory usage merging LoRA to our original model. We could achieve it by `peft` package models with calling `merge_and_unload()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "926e3eae-70b8-46df-acba-6e3960ebecf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2350896ba62848688a5bd263e866889f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    'Phi-3-mini-4k-yoda',\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ").merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab2d219-c0c2-45a7-9a9d-a10ceed3e84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7642.159488\n"
     ]
    }
   ],
   "source": [
    "# check how much space the model takes\n",
    "print(merged_model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4771335b-1d50-4935-94a7-71927767d93c",
   "metadata": {},
   "source": [
    "Only 760MB was used after merging LoRA fine tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6864a-5270-4d21-972c-034668ff1811",
   "metadata": {},
   "source": [
    "Our model could be adapted by `transformers.pipeline` too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "127fc8c7-4198-4e8c-89b8-f5b4b8476283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about star war!.<|end|>\n",
      "<|assistant|>\n",
      " About Star War, tell me something, you must.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about star war!.<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "yoda_pipeline = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "print(yoda_pipeline(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ef0be-c63a-4a61-9fc9-5e281f987cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
